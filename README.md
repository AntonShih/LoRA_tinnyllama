# LoRA_TinyLlama

---

# 目標 : 
在三天內微調出一個對黴菌專家的 AI agent 
- 定義客群 : 
    針對鞋子有黴菌的問題處理
- 這個專家要能回答的三種問題 :
    鞋子有黴菌時 :
        1. 基礎知識
        2. 進階診斷
        3. 處理方案


- 有可能遇到的問題 : 
    1. 速度 : 硬體設備低，要大幅增加訓練速度
    2. 過擬合 : 大幅增加速度可能導致過擬合問題

- 解決辦法 :
    1. 用 dropout 或是提前停止的機制防止過擬合
    2. 使用較小型的 開源模型 : 選用 TinyLlama  (1.1B参數)
    3. 學習率 、 梯度 、 減少權重更新的量
    4. 使用高級語言模型生出的對應集 => 提高訓練數據質量，減少訓練數目

---

# 硬體設施 : 
- GPU : RTX 2050

###  LoRA 重要調配 : 配置更低的試配器權重
- r = 4 選擇較少權重數的更新
- lora_dropout = 0.2 設置較高一點點的 dropout 降低一些過擬合的可能，但不能太高會欠擬合
- bias="none" 不更新偏致參數，加快訓練速度

### 训练参数 重要調配 : 配置更快的學習率將訓練時間縮短
- learning_rate=2e-4  超參數調高加快更新速度
- early_stopping_patience=2  設置提早停止機制避免无效训练
- 使用梯度累積
    batch_size = 4 
    gradient_accumulation_steps = 4  # 累積4次，模擬batch_size=16
- 資料集較少 => 使用交叉驗證

### 資料集設定
- 增加多樣性 : 涵蓋不同類型的黴菌問題和情境
- 階層化組織 : 將資料分為基礎知識、進階診斷、處理方案 3 大類別
    - 在基礎知識加入多樣性設定 : 涵蓋 鞋子黴菌的成因、影響、環境因素、材質差異、人體健康影響
- 結論設定 : 
    1. 100 條基礎知識、100 條進階診斷、100 條處理方案
    2. 按 70%：15%：15% 比例分配至訓練、驗證、測試集

---

# 最差預估結果 :
1. 最差情况：3.5-4小時的訓練時間
2. 模型需繼續微調 => 再使用較小的學習率訓練


--- 
# 時程安排 :
- 第一天 : 
    1. 準備 500 條數據集
    2. 準備 環境 (包含查看訓練狀況圖、保存機制、回朔機制)
    3. 先使用 5 條數據進行訓練測試
    3. 晚上訓練

- 第二天 : 
    1. 察看訓練表現
    2. 處理微調部分
    3. 結束訓練

- 第三天 : 
    1. 部屬 AI agent 
    2. 專案結果對比