{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 7.5,
  "eval_steps": 500,
  "global_step": 750,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.02,
      "grad_norm": 13.800914764404297,
      "learning_rate": 1e-05,
      "loss": 9.0299,
      "step": 2
    },
    {
      "epoch": 0.04,
      "grad_norm": 15.385149002075195,
      "learning_rate": 2e-05,
      "loss": 9.7461,
      "step": 4
    },
    {
      "epoch": 0.06,
      "grad_norm": 14.30284595489502,
      "learning_rate": 3e-05,
      "loss": 9.1391,
      "step": 6
    },
    {
      "epoch": 0.08,
      "grad_norm": 11.796828269958496,
      "learning_rate": 4e-05,
      "loss": 8.6126,
      "step": 8
    },
    {
      "epoch": 0.1,
      "grad_norm": 15.480576515197754,
      "learning_rate": 5e-05,
      "loss": 8.2911,
      "step": 10
    },
    {
      "epoch": 0.12,
      "grad_norm": 14.557957649230957,
      "learning_rate": 6e-05,
      "loss": 7.4342,
      "step": 12
    },
    {
      "epoch": 0.14,
      "grad_norm": 23.80746841430664,
      "learning_rate": 7.000000000000001e-05,
      "loss": 9.0087,
      "step": 14
    },
    {
      "epoch": 0.16,
      "grad_norm": 26.986120223999023,
      "learning_rate": 8e-05,
      "loss": 6.5754,
      "step": 16
    },
    {
      "epoch": 0.18,
      "grad_norm": 33.25328063964844,
      "learning_rate": 8.999999999999999e-05,
      "loss": 5.9501,
      "step": 18
    },
    {
      "epoch": 0.2,
      "grad_norm": 25.66154670715332,
      "learning_rate": 0.0001,
      "loss": 4.252,
      "step": 20
    },
    {
      "epoch": 0.22,
      "grad_norm": 14.371460914611816,
      "learning_rate": 0.00011,
      "loss": 2.8055,
      "step": 22
    },
    {
      "epoch": 0.24,
      "grad_norm": 18.11847686767578,
      "learning_rate": 0.00012,
      "loss": 2.5002,
      "step": 24
    },
    {
      "epoch": 0.26,
      "grad_norm": 17.566028594970703,
      "learning_rate": 0.00013000000000000002,
      "loss": 1.8736,
      "step": 26
    },
    {
      "epoch": 0.28,
      "grad_norm": 4.205204486846924,
      "learning_rate": 0.00014000000000000001,
      "loss": 1.2693,
      "step": 28
    },
    {
      "epoch": 0.3,
      "grad_norm": 12.840989112854004,
      "learning_rate": 0.00015,
      "loss": 1.1053,
      "step": 30
    },
    {
      "epoch": 0.32,
      "grad_norm": 3.381459951400757,
      "learning_rate": 0.00016,
      "loss": 1.0322,
      "step": 32
    },
    {
      "epoch": 0.34,
      "grad_norm": 1.3245962858200073,
      "learning_rate": 0.00017,
      "loss": 0.9703,
      "step": 34
    },
    {
      "epoch": 0.36,
      "grad_norm": 1.070057988166809,
      "learning_rate": 0.00017999999999999998,
      "loss": 0.9304,
      "step": 36
    },
    {
      "epoch": 0.38,
      "grad_norm": 1.0809229612350464,
      "learning_rate": 0.00019,
      "loss": 0.8643,
      "step": 38
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.139465093612671,
      "learning_rate": 0.0002,
      "loss": 0.8868,
      "step": 40
    },
    {
      "epoch": 0.42,
      "grad_norm": 1.0010794401168823,
      "learning_rate": 0.00021,
      "loss": 0.8282,
      "step": 42
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.977220356464386,
      "learning_rate": 0.00022,
      "loss": 0.8517,
      "step": 44
    },
    {
      "epoch": 0.46,
      "grad_norm": 1.0707567930221558,
      "learning_rate": 0.00023,
      "loss": 0.723,
      "step": 46
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.8626884818077087,
      "learning_rate": 0.00024,
      "loss": 0.7861,
      "step": 48
    },
    {
      "epoch": 0.5,
      "grad_norm": 0.940143883228302,
      "learning_rate": 0.00025,
      "loss": 0.7406,
      "step": 50
    },
    {
      "epoch": 0.52,
      "grad_norm": 1.1117761135101318,
      "learning_rate": 0.00026000000000000003,
      "loss": 0.6568,
      "step": 52
    },
    {
      "epoch": 0.54,
      "grad_norm": 0.9589917659759521,
      "learning_rate": 0.00027,
      "loss": 0.7228,
      "step": 54
    },
    {
      "epoch": 0.56,
      "grad_norm": 1.1001994609832764,
      "learning_rate": 0.00028000000000000003,
      "loss": 0.7715,
      "step": 56
    },
    {
      "epoch": 0.58,
      "grad_norm": 1.2325953245162964,
      "learning_rate": 0.00029,
      "loss": 0.9422,
      "step": 58
    },
    {
      "epoch": 0.6,
      "grad_norm": 0.9608914852142334,
      "learning_rate": 0.0003,
      "loss": 0.7331,
      "step": 60
    },
    {
      "epoch": 0.62,
      "grad_norm": 1.0538253784179688,
      "learning_rate": 0.00031,
      "loss": 0.6964,
      "step": 62
    },
    {
      "epoch": 0.64,
      "grad_norm": 1.2104582786560059,
      "learning_rate": 0.00032,
      "loss": 0.7376,
      "step": 64
    },
    {
      "epoch": 0.66,
      "grad_norm": 0.9581096768379211,
      "learning_rate": 0.00033,
      "loss": 0.6976,
      "step": 66
    },
    {
      "epoch": 0.68,
      "grad_norm": 0.9173387289047241,
      "learning_rate": 0.00034,
      "loss": 0.677,
      "step": 68
    },
    {
      "epoch": 0.7,
      "grad_norm": 1.129103183746338,
      "learning_rate": 0.00035,
      "loss": 0.7983,
      "step": 70
    },
    {
      "epoch": 0.72,
      "grad_norm": 1.1328589916229248,
      "learning_rate": 0.00035999999999999997,
      "loss": 0.7199,
      "step": 72
    },
    {
      "epoch": 0.74,
      "grad_norm": 1.041510820388794,
      "learning_rate": 0.00037,
      "loss": 0.6755,
      "step": 74
    },
    {
      "epoch": 0.76,
      "grad_norm": 1.041788935661316,
      "learning_rate": 0.00038,
      "loss": 0.6961,
      "step": 76
    },
    {
      "epoch": 0.78,
      "grad_norm": 1.085530400276184,
      "learning_rate": 0.00039000000000000005,
      "loss": 0.7805,
      "step": 78
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.9955134391784668,
      "learning_rate": 0.0004,
      "loss": 0.653,
      "step": 80
    },
    {
      "epoch": 0.82,
      "grad_norm": 0.881709098815918,
      "learning_rate": 0.00041,
      "loss": 0.6082,
      "step": 82
    },
    {
      "epoch": 0.84,
      "grad_norm": 0.8543424010276794,
      "learning_rate": 0.00042,
      "loss": 0.682,
      "step": 84
    },
    {
      "epoch": 0.86,
      "grad_norm": 1.1659249067306519,
      "learning_rate": 0.00043,
      "loss": 0.6562,
      "step": 86
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.9725323915481567,
      "learning_rate": 0.00044,
      "loss": 0.5671,
      "step": 88
    },
    {
      "epoch": 0.9,
      "grad_norm": 1.0171983242034912,
      "learning_rate": 0.00045000000000000004,
      "loss": 0.6195,
      "step": 90
    },
    {
      "epoch": 0.92,
      "grad_norm": 0.8743643760681152,
      "learning_rate": 0.00046,
      "loss": 0.5851,
      "step": 92
    },
    {
      "epoch": 0.94,
      "grad_norm": 1.1342118978500366,
      "learning_rate": 0.00047,
      "loss": 0.5447,
      "step": 94
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.5372416973114014,
      "learning_rate": 0.00048,
      "loss": 0.6658,
      "step": 96
    },
    {
      "epoch": 0.98,
      "grad_norm": 1.0187662839889526,
      "learning_rate": 0.00049,
      "loss": 0.6504,
      "step": 98
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.8911892175674438,
      "learning_rate": 0.0005,
      "loss": 0.6569,
      "step": 100
    },
    {
      "epoch": 1.02,
      "grad_norm": 1.1827706098556519,
      "learning_rate": 0.0004988888888888889,
      "loss": 0.6263,
      "step": 102
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.7071285843849182,
      "learning_rate": 0.0004977777777777778,
      "loss": 0.4798,
      "step": 104
    },
    {
      "epoch": 1.06,
      "grad_norm": 0.7472258806228638,
      "learning_rate": 0.0004966666666666666,
      "loss": 0.544,
      "step": 106
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.9521583914756775,
      "learning_rate": 0.0004955555555555556,
      "loss": 0.5055,
      "step": 108
    },
    {
      "epoch": 1.1,
      "grad_norm": 1.4907761812210083,
      "learning_rate": 0.0004944444444444445,
      "loss": 0.6103,
      "step": 110
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.8278586864471436,
      "learning_rate": 0.0004933333333333334,
      "loss": 0.4961,
      "step": 112
    },
    {
      "epoch": 1.14,
      "grad_norm": 0.9533631205558777,
      "learning_rate": 0.0004922222222222222,
      "loss": 0.5072,
      "step": 114
    },
    {
      "epoch": 1.16,
      "grad_norm": 1.134857416152954,
      "learning_rate": 0.0004911111111111111,
      "loss": 0.5923,
      "step": 116
    },
    {
      "epoch": 1.18,
      "grad_norm": 0.9268627166748047,
      "learning_rate": 0.00049,
      "loss": 0.6206,
      "step": 118
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.9752512574195862,
      "learning_rate": 0.0004888888888888889,
      "loss": 0.4896,
      "step": 120
    },
    {
      "epoch": 1.22,
      "grad_norm": 1.0902912616729736,
      "learning_rate": 0.0004877777777777778,
      "loss": 0.643,
      "step": 122
    },
    {
      "epoch": 1.24,
      "grad_norm": 0.9164211750030518,
      "learning_rate": 0.0004866666666666667,
      "loss": 0.5399,
      "step": 124
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.8965872526168823,
      "learning_rate": 0.0004855555555555556,
      "loss": 0.5894,
      "step": 126
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.8417169451713562,
      "learning_rate": 0.00048444444444444446,
      "loss": 0.507,
      "step": 128
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.8446664214134216,
      "learning_rate": 0.00048333333333333334,
      "loss": 0.4528,
      "step": 130
    },
    {
      "epoch": 1.32,
      "grad_norm": 1.0075205564498901,
      "learning_rate": 0.0004822222222222222,
      "loss": 0.626,
      "step": 132
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.8968895077705383,
      "learning_rate": 0.0004811111111111111,
      "loss": 0.6127,
      "step": 134
    },
    {
      "epoch": 1.36,
      "grad_norm": 1.1152961254119873,
      "learning_rate": 0.00048,
      "loss": 0.5321,
      "step": 136
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.9310101866722107,
      "learning_rate": 0.0004788888888888889,
      "loss": 0.5123,
      "step": 138
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.9740699529647827,
      "learning_rate": 0.0004777777777777778,
      "loss": 0.5215,
      "step": 140
    },
    {
      "epoch": 1.42,
      "grad_norm": 0.8971349596977234,
      "learning_rate": 0.0004766666666666667,
      "loss": 0.6252,
      "step": 142
    },
    {
      "epoch": 1.44,
      "grad_norm": 1.1177704334259033,
      "learning_rate": 0.00047555555555555556,
      "loss": 0.6293,
      "step": 144
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.9184234738349915,
      "learning_rate": 0.00047444444444444444,
      "loss": 0.4552,
      "step": 146
    },
    {
      "epoch": 1.48,
      "grad_norm": 1.0250598192214966,
      "learning_rate": 0.00047333333333333336,
      "loss": 0.6774,
      "step": 148
    },
    {
      "epoch": 1.5,
      "grad_norm": 0.772085964679718,
      "learning_rate": 0.00047222222222222224,
      "loss": 0.5782,
      "step": 150
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.7720305919647217,
      "learning_rate": 0.0004711111111111111,
      "loss": 0.4552,
      "step": 152
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.7730128169059753,
      "learning_rate": 0.00047,
      "loss": 0.5374,
      "step": 154
    },
    {
      "epoch": 1.56,
      "grad_norm": 1.0617316961288452,
      "learning_rate": 0.0004688888888888889,
      "loss": 0.4834,
      "step": 156
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.7253595590591431,
      "learning_rate": 0.0004677777777777778,
      "loss": 0.4863,
      "step": 158
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.9587801098823547,
      "learning_rate": 0.00046666666666666666,
      "loss": 0.552,
      "step": 160
    },
    {
      "epoch": 1.62,
      "grad_norm": 0.8960451483726501,
      "learning_rate": 0.0004655555555555556,
      "loss": 0.5417,
      "step": 162
    },
    {
      "epoch": 1.64,
      "grad_norm": 1.1200928688049316,
      "learning_rate": 0.00046444444444444446,
      "loss": 0.553,
      "step": 164
    },
    {
      "epoch": 1.66,
      "grad_norm": 1.0428402423858643,
      "learning_rate": 0.00046333333333333334,
      "loss": 0.5232,
      "step": 166
    },
    {
      "epoch": 1.68,
      "grad_norm": 1.278228759765625,
      "learning_rate": 0.0004622222222222222,
      "loss": 0.5695,
      "step": 168
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.7833808660507202,
      "learning_rate": 0.00046111111111111114,
      "loss": 0.4444,
      "step": 170
    },
    {
      "epoch": 1.72,
      "grad_norm": 0.902855396270752,
      "learning_rate": 0.00046,
      "loss": 0.5164,
      "step": 172
    },
    {
      "epoch": 1.74,
      "grad_norm": 1.0241332054138184,
      "learning_rate": 0.0004588888888888889,
      "loss": 0.516,
      "step": 174
    },
    {
      "epoch": 1.76,
      "grad_norm": 1.0135363340377808,
      "learning_rate": 0.0004577777777777778,
      "loss": 0.4845,
      "step": 176
    },
    {
      "epoch": 1.78,
      "grad_norm": 0.9168480038642883,
      "learning_rate": 0.0004566666666666667,
      "loss": 0.4518,
      "step": 178
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.8879546523094177,
      "learning_rate": 0.00045555555555555556,
      "loss": 0.4367,
      "step": 180
    },
    {
      "epoch": 1.82,
      "grad_norm": 0.8626773357391357,
      "learning_rate": 0.00045444444444444444,
      "loss": 0.416,
      "step": 182
    },
    {
      "epoch": 1.84,
      "grad_norm": 0.9781126379966736,
      "learning_rate": 0.0004533333333333333,
      "loss": 0.5687,
      "step": 184
    },
    {
      "epoch": 1.86,
      "grad_norm": 1.0991120338439941,
      "learning_rate": 0.00045222222222222224,
      "loss": 0.5547,
      "step": 186
    },
    {
      "epoch": 1.88,
      "grad_norm": 1.4910938739776611,
      "learning_rate": 0.0004511111111111111,
      "loss": 0.565,
      "step": 188
    },
    {
      "epoch": 1.9,
      "grad_norm": 1.2236055135726929,
      "learning_rate": 0.00045000000000000004,
      "loss": 0.5825,
      "step": 190
    },
    {
      "epoch": 1.92,
      "grad_norm": 1.1031441688537598,
      "learning_rate": 0.0004488888888888889,
      "loss": 0.6036,
      "step": 192
    },
    {
      "epoch": 1.94,
      "grad_norm": 0.8682137727737427,
      "learning_rate": 0.0004477777777777778,
      "loss": 0.5514,
      "step": 194
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.9597962498664856,
      "learning_rate": 0.00044666666666666666,
      "loss": 0.4579,
      "step": 196
    },
    {
      "epoch": 1.98,
      "grad_norm": 0.8162915706634521,
      "learning_rate": 0.00044555555555555554,
      "loss": 0.4479,
      "step": 198
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.8522354960441589,
      "learning_rate": 0.0004444444444444444,
      "loss": 0.4941,
      "step": 200
    },
    {
      "epoch": 2.02,
      "grad_norm": 0.6780248284339905,
      "learning_rate": 0.00044333333333333334,
      "loss": 0.3647,
      "step": 202
    },
    {
      "epoch": 2.04,
      "grad_norm": 0.6801981925964355,
      "learning_rate": 0.00044222222222222227,
      "loss": 0.3922,
      "step": 204
    },
    {
      "epoch": 2.06,
      "grad_norm": 0.7540828585624695,
      "learning_rate": 0.00044111111111111114,
      "loss": 0.3372,
      "step": 206
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.8450609445571899,
      "learning_rate": 0.00044,
      "loss": 0.4169,
      "step": 208
    },
    {
      "epoch": 2.1,
      "grad_norm": 0.9007397294044495,
      "learning_rate": 0.0004388888888888889,
      "loss": 0.4708,
      "step": 210
    },
    {
      "epoch": 2.12,
      "grad_norm": 0.8964400887489319,
      "learning_rate": 0.00043777777777777776,
      "loss": 0.3249,
      "step": 212
    },
    {
      "epoch": 2.14,
      "grad_norm": 0.9236611723899841,
      "learning_rate": 0.00043666666666666664,
      "loss": 0.4216,
      "step": 214
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.8443900346755981,
      "learning_rate": 0.0004355555555555555,
      "loss": 0.436,
      "step": 216
    },
    {
      "epoch": 2.18,
      "grad_norm": 1.1168385744094849,
      "learning_rate": 0.0004344444444444445,
      "loss": 0.4777,
      "step": 218
    },
    {
      "epoch": 2.2,
      "grad_norm": 1.1418678760528564,
      "learning_rate": 0.00043333333333333337,
      "loss": 0.3685,
      "step": 220
    },
    {
      "epoch": 2.22,
      "grad_norm": 0.8788106441497803,
      "learning_rate": 0.00043222222222222224,
      "loss": 0.3462,
      "step": 222
    },
    {
      "epoch": 2.24,
      "grad_norm": 1.2888219356536865,
      "learning_rate": 0.0004311111111111111,
      "loss": 0.4673,
      "step": 224
    },
    {
      "epoch": 2.26,
      "grad_norm": 0.885313868522644,
      "learning_rate": 0.00043,
      "loss": 0.3117,
      "step": 226
    },
    {
      "epoch": 2.28,
      "grad_norm": 0.982437789440155,
      "learning_rate": 0.00042888888888888886,
      "loss": 0.4223,
      "step": 228
    },
    {
      "epoch": 2.3,
      "grad_norm": 0.8671413064002991,
      "learning_rate": 0.0004277777777777778,
      "loss": 0.4109,
      "step": 230
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.8446153402328491,
      "learning_rate": 0.0004266666666666667,
      "loss": 0.3462,
      "step": 232
    },
    {
      "epoch": 2.34,
      "grad_norm": 0.8006841540336609,
      "learning_rate": 0.0004255555555555556,
      "loss": 0.3751,
      "step": 234
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.9032877683639526,
      "learning_rate": 0.00042444444444444447,
      "loss": 0.3791,
      "step": 236
    },
    {
      "epoch": 2.38,
      "grad_norm": 0.8068385720252991,
      "learning_rate": 0.00042333333333333334,
      "loss": 0.3447,
      "step": 238
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.9784874320030212,
      "learning_rate": 0.0004222222222222222,
      "loss": 0.3861,
      "step": 240
    },
    {
      "epoch": 2.42,
      "grad_norm": 0.8090668320655823,
      "learning_rate": 0.0004211111111111111,
      "loss": 0.3653,
      "step": 242
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.9999850392341614,
      "learning_rate": 0.00042,
      "loss": 0.4753,
      "step": 244
    },
    {
      "epoch": 2.46,
      "grad_norm": 1.2101694345474243,
      "learning_rate": 0.0004188888888888889,
      "loss": 0.443,
      "step": 246
    },
    {
      "epoch": 2.48,
      "grad_norm": 1.0100421905517578,
      "learning_rate": 0.0004177777777777778,
      "loss": 0.4641,
      "step": 248
    },
    {
      "epoch": 2.5,
      "grad_norm": 1.1222037076950073,
      "learning_rate": 0.0004166666666666667,
      "loss": 0.4052,
      "step": 250
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.9581074118614197,
      "learning_rate": 0.00041555555555555557,
      "loss": 0.4268,
      "step": 252
    },
    {
      "epoch": 2.54,
      "grad_norm": 1.029875636100769,
      "learning_rate": 0.00041444444444444444,
      "loss": 0.3813,
      "step": 254
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.9951274991035461,
      "learning_rate": 0.0004133333333333333,
      "loss": 0.43,
      "step": 256
    },
    {
      "epoch": 2.58,
      "grad_norm": 0.9655263423919678,
      "learning_rate": 0.00041222222222222224,
      "loss": 0.3118,
      "step": 258
    },
    {
      "epoch": 2.6,
      "grad_norm": 1.20745849609375,
      "learning_rate": 0.0004111111111111111,
      "loss": 0.4202,
      "step": 260
    },
    {
      "epoch": 2.62,
      "grad_norm": 1.0401320457458496,
      "learning_rate": 0.00041,
      "loss": 0.3717,
      "step": 262
    },
    {
      "epoch": 2.64,
      "grad_norm": 1.0434271097183228,
      "learning_rate": 0.0004088888888888889,
      "loss": 0.4225,
      "step": 264
    },
    {
      "epoch": 2.66,
      "grad_norm": 1.3127847909927368,
      "learning_rate": 0.0004077777777777778,
      "loss": 0.4305,
      "step": 266
    },
    {
      "epoch": 2.68,
      "grad_norm": 1.0665216445922852,
      "learning_rate": 0.00040666666666666667,
      "loss": 0.4108,
      "step": 268
    },
    {
      "epoch": 2.7,
      "grad_norm": 1.0378625392913818,
      "learning_rate": 0.00040555555555555554,
      "loss": 0.3818,
      "step": 270
    },
    {
      "epoch": 2.72,
      "grad_norm": 1.0531294345855713,
      "learning_rate": 0.00040444444444444447,
      "loss": 0.4674,
      "step": 272
    },
    {
      "epoch": 2.74,
      "grad_norm": 0.8795492649078369,
      "learning_rate": 0.00040333333333333334,
      "loss": 0.3404,
      "step": 274
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.9041973948478699,
      "learning_rate": 0.0004022222222222222,
      "loss": 0.3615,
      "step": 276
    },
    {
      "epoch": 2.78,
      "grad_norm": 0.8892728090286255,
      "learning_rate": 0.0004011111111111111,
      "loss": 0.3455,
      "step": 278
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.8543305993080139,
      "learning_rate": 0.0004,
      "loss": 0.3121,
      "step": 280
    },
    {
      "epoch": 2.82,
      "grad_norm": 0.7765733599662781,
      "learning_rate": 0.0003988888888888889,
      "loss": 0.3034,
      "step": 282
    },
    {
      "epoch": 2.84,
      "grad_norm": 1.0380100011825562,
      "learning_rate": 0.00039777777777777777,
      "loss": 0.3779,
      "step": 284
    },
    {
      "epoch": 2.86,
      "grad_norm": 0.9675846695899963,
      "learning_rate": 0.0003966666666666667,
      "loss": 0.4465,
      "step": 286
    },
    {
      "epoch": 2.88,
      "grad_norm": 1.1783618927001953,
      "learning_rate": 0.00039555555555555557,
      "loss": 0.4707,
      "step": 288
    },
    {
      "epoch": 2.9,
      "grad_norm": 0.9495849013328552,
      "learning_rate": 0.00039444444444444444,
      "loss": 0.3602,
      "step": 290
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.9391235709190369,
      "learning_rate": 0.0003933333333333333,
      "loss": 0.327,
      "step": 292
    },
    {
      "epoch": 2.94,
      "grad_norm": 0.8750551342964172,
      "learning_rate": 0.00039222222222222225,
      "loss": 0.3713,
      "step": 294
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.9234009981155396,
      "learning_rate": 0.0003911111111111111,
      "loss": 0.3541,
      "step": 296
    },
    {
      "epoch": 2.98,
      "grad_norm": 0.9162778854370117,
      "learning_rate": 0.00039000000000000005,
      "loss": 0.3587,
      "step": 298
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.8268438577651978,
      "learning_rate": 0.0003888888888888889,
      "loss": 0.3142,
      "step": 300
    },
    {
      "epoch": 3.02,
      "grad_norm": 0.8267666101455688,
      "learning_rate": 0.0003877777777777778,
      "loss": 0.3531,
      "step": 302
    },
    {
      "epoch": 3.04,
      "grad_norm": 0.8743186593055725,
      "learning_rate": 0.00038666666666666667,
      "loss": 0.2966,
      "step": 304
    },
    {
      "epoch": 3.06,
      "grad_norm": 0.5905437469482422,
      "learning_rate": 0.00038555555555555554,
      "loss": 0.2029,
      "step": 306
    },
    {
      "epoch": 3.08,
      "grad_norm": 0.883622944355011,
      "learning_rate": 0.0003844444444444444,
      "loss": 0.2488,
      "step": 308
    },
    {
      "epoch": 3.1,
      "grad_norm": 0.8733870983123779,
      "learning_rate": 0.00038333333333333334,
      "loss": 0.2547,
      "step": 310
    },
    {
      "epoch": 3.12,
      "grad_norm": 0.9249992370605469,
      "learning_rate": 0.0003822222222222223,
      "loss": 0.2836,
      "step": 312
    },
    {
      "epoch": 3.14,
      "grad_norm": 0.9941602349281311,
      "learning_rate": 0.00038111111111111115,
      "loss": 0.2668,
      "step": 314
    },
    {
      "epoch": 3.16,
      "grad_norm": 0.8907420039176941,
      "learning_rate": 0.00038,
      "loss": 0.2242,
      "step": 316
    },
    {
      "epoch": 3.18,
      "grad_norm": 1.0474746227264404,
      "learning_rate": 0.0003788888888888889,
      "loss": 0.2654,
      "step": 318
    },
    {
      "epoch": 3.2,
      "grad_norm": 1.1089977025985718,
      "learning_rate": 0.00037777777777777777,
      "loss": 0.3111,
      "step": 320
    },
    {
      "epoch": 3.22,
      "grad_norm": 0.7304494976997375,
      "learning_rate": 0.00037666666666666664,
      "loss": 0.2646,
      "step": 322
    },
    {
      "epoch": 3.24,
      "grad_norm": 0.9659769535064697,
      "learning_rate": 0.0003755555555555555,
      "loss": 0.2833,
      "step": 324
    },
    {
      "epoch": 3.26,
      "grad_norm": 1.0157208442687988,
      "learning_rate": 0.0003744444444444445,
      "loss": 0.2598,
      "step": 326
    },
    {
      "epoch": 3.28,
      "grad_norm": 1.0036303997039795,
      "learning_rate": 0.0003733333333333334,
      "loss": 0.2497,
      "step": 328
    },
    {
      "epoch": 3.3,
      "grad_norm": 1.0485937595367432,
      "learning_rate": 0.00037222222222222225,
      "loss": 0.2578,
      "step": 330
    },
    {
      "epoch": 3.32,
      "grad_norm": 1.00243079662323,
      "learning_rate": 0.0003711111111111111,
      "loss": 0.2433,
      "step": 332
    },
    {
      "epoch": 3.34,
      "grad_norm": 0.8334999680519104,
      "learning_rate": 0.00037,
      "loss": 0.2749,
      "step": 334
    },
    {
      "epoch": 3.36,
      "grad_norm": 1.2482774257659912,
      "learning_rate": 0.00036888888888888887,
      "loss": 0.2955,
      "step": 336
    },
    {
      "epoch": 3.38,
      "grad_norm": 1.0281882286071777,
      "learning_rate": 0.00036777777777777774,
      "loss": 0.3297,
      "step": 338
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.8382747173309326,
      "learning_rate": 0.00036666666666666667,
      "loss": 0.2091,
      "step": 340
    },
    {
      "epoch": 3.42,
      "grad_norm": 0.8282554149627686,
      "learning_rate": 0.0003655555555555556,
      "loss": 0.2305,
      "step": 342
    },
    {
      "epoch": 3.44,
      "grad_norm": 0.9357958436012268,
      "learning_rate": 0.00036444444444444447,
      "loss": 0.2547,
      "step": 344
    },
    {
      "epoch": 3.46,
      "grad_norm": 1.1434541940689087,
      "learning_rate": 0.00036333333333333335,
      "loss": 0.3639,
      "step": 346
    },
    {
      "epoch": 3.48,
      "grad_norm": 1.0925501585006714,
      "learning_rate": 0.0003622222222222222,
      "loss": 0.287,
      "step": 348
    },
    {
      "epoch": 3.5,
      "grad_norm": 0.907752513885498,
      "learning_rate": 0.0003611111111111111,
      "loss": 0.2193,
      "step": 350
    },
    {
      "epoch": 3.52,
      "grad_norm": 1.3591407537460327,
      "learning_rate": 0.00035999999999999997,
      "loss": 0.3424,
      "step": 352
    },
    {
      "epoch": 3.54,
      "grad_norm": 0.8993517756462097,
      "learning_rate": 0.0003588888888888889,
      "loss": 0.2829,
      "step": 354
    },
    {
      "epoch": 3.56,
      "grad_norm": 0.7474104762077332,
      "learning_rate": 0.00035777777777777777,
      "loss": 0.3012,
      "step": 356
    },
    {
      "epoch": 3.58,
      "grad_norm": 0.952904999256134,
      "learning_rate": 0.0003566666666666667,
      "loss": 0.3181,
      "step": 358
    },
    {
      "epoch": 3.6,
      "grad_norm": 1.206925868988037,
      "learning_rate": 0.00035555555555555557,
      "loss": 0.3155,
      "step": 360
    },
    {
      "epoch": 3.62,
      "grad_norm": 0.8916686773300171,
      "learning_rate": 0.00035444444444444445,
      "loss": 0.2865,
      "step": 362
    },
    {
      "epoch": 3.64,
      "grad_norm": 1.0072802305221558,
      "learning_rate": 0.0003533333333333333,
      "loss": 0.356,
      "step": 364
    },
    {
      "epoch": 3.66,
      "grad_norm": 1.0409618616104126,
      "learning_rate": 0.00035222222222222225,
      "loss": 0.2746,
      "step": 366
    },
    {
      "epoch": 3.68,
      "grad_norm": 1.0900182723999023,
      "learning_rate": 0.0003511111111111111,
      "loss": 0.3431,
      "step": 368
    },
    {
      "epoch": 3.7,
      "grad_norm": 1.0253037214279175,
      "learning_rate": 0.00035,
      "loss": 0.2679,
      "step": 370
    },
    {
      "epoch": 3.72,
      "grad_norm": 0.9525346755981445,
      "learning_rate": 0.0003488888888888889,
      "loss": 0.3597,
      "step": 372
    },
    {
      "epoch": 3.74,
      "grad_norm": 0.9645875692367554,
      "learning_rate": 0.0003477777777777778,
      "loss": 0.2902,
      "step": 374
    },
    {
      "epoch": 3.76,
      "grad_norm": 1.161912202835083,
      "learning_rate": 0.00034666666666666667,
      "loss": 0.3268,
      "step": 376
    },
    {
      "epoch": 3.78,
      "grad_norm": 0.8415015339851379,
      "learning_rate": 0.00034555555555555555,
      "loss": 0.282,
      "step": 378
    },
    {
      "epoch": 3.8,
      "grad_norm": 0.9916043281555176,
      "learning_rate": 0.0003444444444444445,
      "loss": 0.3095,
      "step": 380
    },
    {
      "epoch": 3.82,
      "grad_norm": 1.0486263036727905,
      "learning_rate": 0.00034333333333333335,
      "loss": 0.2719,
      "step": 382
    },
    {
      "epoch": 3.84,
      "grad_norm": 0.8553629517555237,
      "learning_rate": 0.0003422222222222222,
      "loss": 0.214,
      "step": 384
    },
    {
      "epoch": 3.86,
      "grad_norm": 1.068358302116394,
      "learning_rate": 0.0003411111111111111,
      "loss": 0.2846,
      "step": 386
    },
    {
      "epoch": 3.88,
      "grad_norm": 0.9065567851066589,
      "learning_rate": 0.00034,
      "loss": 0.2287,
      "step": 388
    },
    {
      "epoch": 3.9,
      "grad_norm": 0.9064179062843323,
      "learning_rate": 0.0003388888888888889,
      "loss": 0.2418,
      "step": 390
    },
    {
      "epoch": 3.92,
      "grad_norm": 0.9413698315620422,
      "learning_rate": 0.00033777777777777777,
      "loss": 0.2938,
      "step": 392
    },
    {
      "epoch": 3.94,
      "grad_norm": 1.0954188108444214,
      "learning_rate": 0.0003366666666666667,
      "loss": 0.2771,
      "step": 394
    },
    {
      "epoch": 3.96,
      "grad_norm": 1.1413217782974243,
      "learning_rate": 0.0003355555555555556,
      "loss": 0.3329,
      "step": 396
    },
    {
      "epoch": 3.98,
      "grad_norm": 0.9327134490013123,
      "learning_rate": 0.00033444444444444445,
      "loss": 0.2723,
      "step": 398
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.1706252098083496,
      "learning_rate": 0.0003333333333333333,
      "loss": 0.285,
      "step": 400
    },
    {
      "epoch": 4.02,
      "grad_norm": 0.797343909740448,
      "learning_rate": 0.0003322222222222222,
      "loss": 0.186,
      "step": 402
    },
    {
      "epoch": 4.04,
      "grad_norm": 0.9140872955322266,
      "learning_rate": 0.0003311111111111111,
      "loss": 0.2623,
      "step": 404
    },
    {
      "epoch": 4.06,
      "grad_norm": 1.0779595375061035,
      "learning_rate": 0.00033,
      "loss": 0.2135,
      "step": 406
    },
    {
      "epoch": 4.08,
      "grad_norm": 0.8020479083061218,
      "learning_rate": 0.0003288888888888889,
      "loss": 0.1899,
      "step": 408
    },
    {
      "epoch": 4.1,
      "grad_norm": 0.7965507507324219,
      "learning_rate": 0.0003277777777777778,
      "loss": 0.1904,
      "step": 410
    },
    {
      "epoch": 4.12,
      "grad_norm": 1.5730711221694946,
      "learning_rate": 0.0003266666666666667,
      "loss": 0.2521,
      "step": 412
    },
    {
      "epoch": 4.14,
      "grad_norm": 0.9620688557624817,
      "learning_rate": 0.00032555555555555555,
      "loss": 0.2269,
      "step": 414
    },
    {
      "epoch": 4.16,
      "grad_norm": 0.9693943858146667,
      "learning_rate": 0.0003244444444444444,
      "loss": 0.2125,
      "step": 416
    },
    {
      "epoch": 4.18,
      "grad_norm": 0.9991424083709717,
      "learning_rate": 0.0003233333333333333,
      "loss": 0.1649,
      "step": 418
    },
    {
      "epoch": 4.2,
      "grad_norm": 0.9475535750389099,
      "learning_rate": 0.0003222222222222222,
      "loss": 0.176,
      "step": 420
    },
    {
      "epoch": 4.22,
      "grad_norm": 1.1401431560516357,
      "learning_rate": 0.00032111111111111115,
      "loss": 0.1822,
      "step": 422
    },
    {
      "epoch": 4.24,
      "grad_norm": 0.8012798428535461,
      "learning_rate": 0.00032,
      "loss": 0.1477,
      "step": 424
    },
    {
      "epoch": 4.26,
      "grad_norm": 0.9034323692321777,
      "learning_rate": 0.0003188888888888889,
      "loss": 0.1794,
      "step": 426
    },
    {
      "epoch": 4.28,
      "grad_norm": 0.9704189896583557,
      "learning_rate": 0.0003177777777777778,
      "loss": 0.1727,
      "step": 428
    },
    {
      "epoch": 4.3,
      "grad_norm": 0.8871170878410339,
      "learning_rate": 0.00031666666666666665,
      "loss": 0.1818,
      "step": 430
    },
    {
      "epoch": 4.32,
      "grad_norm": 1.0970540046691895,
      "learning_rate": 0.0003155555555555555,
      "loss": 0.196,
      "step": 432
    },
    {
      "epoch": 4.34,
      "grad_norm": 0.8625584244728088,
      "learning_rate": 0.0003144444444444445,
      "loss": 0.2286,
      "step": 434
    },
    {
      "epoch": 4.36,
      "grad_norm": 1.115256667137146,
      "learning_rate": 0.0003133333333333334,
      "loss": 0.1991,
      "step": 436
    },
    {
      "epoch": 4.38,
      "grad_norm": 0.7814847230911255,
      "learning_rate": 0.00031222222222222225,
      "loss": 0.1793,
      "step": 438
    },
    {
      "epoch": 4.4,
      "grad_norm": 1.2203978300094604,
      "learning_rate": 0.0003111111111111111,
      "loss": 0.1605,
      "step": 440
    },
    {
      "epoch": 4.42,
      "grad_norm": 0.9131170511245728,
      "learning_rate": 0.00031,
      "loss": 0.2019,
      "step": 442
    },
    {
      "epoch": 4.44,
      "grad_norm": 0.7654973864555359,
      "learning_rate": 0.0003088888888888889,
      "loss": 0.1773,
      "step": 444
    },
    {
      "epoch": 4.46,
      "grad_norm": 0.6621642708778381,
      "learning_rate": 0.00030777777777777775,
      "loss": 0.1754,
      "step": 446
    },
    {
      "epoch": 4.48,
      "grad_norm": 0.8417531847953796,
      "learning_rate": 0.0003066666666666667,
      "loss": 0.1747,
      "step": 448
    },
    {
      "epoch": 4.5,
      "grad_norm": 1.268955945968628,
      "learning_rate": 0.0003055555555555556,
      "loss": 0.1947,
      "step": 450
    },
    {
      "epoch": 4.52,
      "grad_norm": 0.9074463844299316,
      "learning_rate": 0.0003044444444444445,
      "loss": 0.1615,
      "step": 452
    },
    {
      "epoch": 4.54,
      "grad_norm": 1.2913134098052979,
      "learning_rate": 0.00030333333333333335,
      "loss": 0.1761,
      "step": 454
    },
    {
      "epoch": 4.56,
      "grad_norm": 1.235394835472107,
      "learning_rate": 0.0003022222222222222,
      "loss": 0.2486,
      "step": 456
    },
    {
      "epoch": 4.58,
      "grad_norm": 0.9313056468963623,
      "learning_rate": 0.0003011111111111111,
      "loss": 0.2057,
      "step": 458
    },
    {
      "epoch": 4.6,
      "grad_norm": 1.2075780630111694,
      "learning_rate": 0.0003,
      "loss": 0.2069,
      "step": 460
    },
    {
      "epoch": 4.62,
      "grad_norm": 0.9773450493812561,
      "learning_rate": 0.0002988888888888889,
      "loss": 0.1855,
      "step": 462
    },
    {
      "epoch": 4.64,
      "grad_norm": 1.1872344017028809,
      "learning_rate": 0.0002977777777777778,
      "loss": 0.2034,
      "step": 464
    },
    {
      "epoch": 4.66,
      "grad_norm": 1.2306864261627197,
      "learning_rate": 0.0002966666666666667,
      "loss": 0.2081,
      "step": 466
    },
    {
      "epoch": 4.68,
      "grad_norm": 1.1068165302276611,
      "learning_rate": 0.0002955555555555556,
      "loss": 0.2114,
      "step": 468
    },
    {
      "epoch": 4.7,
      "grad_norm": 0.6772137880325317,
      "learning_rate": 0.00029444444444444445,
      "loss": 0.1624,
      "step": 470
    },
    {
      "epoch": 4.72,
      "grad_norm": 0.8490572571754456,
      "learning_rate": 0.0002933333333333333,
      "loss": 0.2165,
      "step": 472
    },
    {
      "epoch": 4.74,
      "grad_norm": 0.917948305606842,
      "learning_rate": 0.0002922222222222222,
      "loss": 0.2027,
      "step": 474
    },
    {
      "epoch": 4.76,
      "grad_norm": 1.1025117635726929,
      "learning_rate": 0.00029111111111111113,
      "loss": 0.2206,
      "step": 476
    },
    {
      "epoch": 4.78,
      "grad_norm": 0.9286566972732544,
      "learning_rate": 0.00029,
      "loss": 0.1885,
      "step": 478
    },
    {
      "epoch": 4.8,
      "grad_norm": 1.0626423358917236,
      "learning_rate": 0.0002888888888888889,
      "loss": 0.2193,
      "step": 480
    },
    {
      "epoch": 4.82,
      "grad_norm": 1.0861228704452515,
      "learning_rate": 0.0002877777777777778,
      "loss": 0.1924,
      "step": 482
    },
    {
      "epoch": 4.84,
      "grad_norm": 1.2214797735214233,
      "learning_rate": 0.0002866666666666667,
      "loss": 0.2295,
      "step": 484
    },
    {
      "epoch": 4.86,
      "grad_norm": 0.9295117259025574,
      "learning_rate": 0.00028555555555555555,
      "loss": 0.2011,
      "step": 486
    },
    {
      "epoch": 4.88,
      "grad_norm": 0.8912417888641357,
      "learning_rate": 0.0002844444444444444,
      "loss": 0.1757,
      "step": 488
    },
    {
      "epoch": 4.9,
      "grad_norm": 1.0340932607650757,
      "learning_rate": 0.00028333333333333335,
      "loss": 0.2147,
      "step": 490
    },
    {
      "epoch": 4.92,
      "grad_norm": 1.058254599571228,
      "learning_rate": 0.00028222222222222223,
      "loss": 0.2095,
      "step": 492
    },
    {
      "epoch": 4.94,
      "grad_norm": 1.1201326847076416,
      "learning_rate": 0.0002811111111111111,
      "loss": 0.1855,
      "step": 494
    },
    {
      "epoch": 4.96,
      "grad_norm": 0.9144668579101562,
      "learning_rate": 0.00028000000000000003,
      "loss": 0.1785,
      "step": 496
    },
    {
      "epoch": 4.98,
      "grad_norm": 0.870752215385437,
      "learning_rate": 0.0002788888888888889,
      "loss": 0.1553,
      "step": 498
    },
    {
      "epoch": 5.0,
      "grad_norm": 1.2348262071609497,
      "learning_rate": 0.0002777777777777778,
      "loss": 0.2707,
      "step": 500
    },
    {
      "epoch": 5.02,
      "grad_norm": 0.6897755861282349,
      "learning_rate": 0.00027666666666666665,
      "loss": 0.1281,
      "step": 502
    },
    {
      "epoch": 5.04,
      "grad_norm": 0.6788140535354614,
      "learning_rate": 0.0002755555555555556,
      "loss": 0.1246,
      "step": 504
    },
    {
      "epoch": 5.06,
      "grad_norm": 0.929266631603241,
      "learning_rate": 0.00027444444444444445,
      "loss": 0.1454,
      "step": 506
    },
    {
      "epoch": 5.08,
      "grad_norm": 0.9576708674430847,
      "learning_rate": 0.00027333333333333333,
      "loss": 0.1374,
      "step": 508
    },
    {
      "epoch": 5.1,
      "grad_norm": 0.7407622337341309,
      "learning_rate": 0.0002722222222222222,
      "loss": 0.1353,
      "step": 510
    },
    {
      "epoch": 5.12,
      "grad_norm": 0.7658826112747192,
      "learning_rate": 0.00027111111111111113,
      "loss": 0.1555,
      "step": 512
    },
    {
      "epoch": 5.14,
      "grad_norm": 0.6357641816139221,
      "learning_rate": 0.00027,
      "loss": 0.1209,
      "step": 514
    },
    {
      "epoch": 5.16,
      "grad_norm": 1.0212337970733643,
      "learning_rate": 0.00026888888888888893,
      "loss": 0.1393,
      "step": 516
    },
    {
      "epoch": 5.18,
      "grad_norm": 0.966454029083252,
      "learning_rate": 0.0002677777777777778,
      "loss": 0.1194,
      "step": 518
    },
    {
      "epoch": 5.2,
      "grad_norm": 1.1409326791763306,
      "learning_rate": 0.0002666666666666667,
      "loss": 0.1207,
      "step": 520
    },
    {
      "epoch": 5.22,
      "grad_norm": 0.8780993819236755,
      "learning_rate": 0.00026555555555555555,
      "loss": 0.1185,
      "step": 522
    },
    {
      "epoch": 5.24,
      "grad_norm": 0.6928815841674805,
      "learning_rate": 0.00026444444444444443,
      "loss": 0.1196,
      "step": 524
    },
    {
      "epoch": 5.26,
      "grad_norm": 1.038499355316162,
      "learning_rate": 0.0002633333333333333,
      "loss": 0.1021,
      "step": 526
    },
    {
      "epoch": 5.28,
      "grad_norm": 0.6632353067398071,
      "learning_rate": 0.00026222222222222223,
      "loss": 0.1144,
      "step": 528
    },
    {
      "epoch": 5.3,
      "grad_norm": 0.918925940990448,
      "learning_rate": 0.00026111111111111116,
      "loss": 0.1419,
      "step": 530
    },
    {
      "epoch": 5.32,
      "grad_norm": 0.8991079926490784,
      "learning_rate": 0.00026000000000000003,
      "loss": 0.0905,
      "step": 532
    },
    {
      "epoch": 5.34,
      "grad_norm": 0.9704847931861877,
      "learning_rate": 0.0002588888888888889,
      "loss": 0.1223,
      "step": 534
    },
    {
      "epoch": 5.36,
      "grad_norm": 1.092423677444458,
      "learning_rate": 0.0002577777777777778,
      "loss": 0.1373,
      "step": 536
    },
    {
      "epoch": 5.38,
      "grad_norm": 0.824557900428772,
      "learning_rate": 0.00025666666666666665,
      "loss": 0.1262,
      "step": 538
    },
    {
      "epoch": 5.4,
      "grad_norm": 0.9613047242164612,
      "learning_rate": 0.00025555555555555553,
      "loss": 0.136,
      "step": 540
    },
    {
      "epoch": 5.42,
      "grad_norm": 0.8230829834938049,
      "learning_rate": 0.0002544444444444444,
      "loss": 0.1143,
      "step": 542
    },
    {
      "epoch": 5.44,
      "grad_norm": 0.6744479537010193,
      "learning_rate": 0.0002533333333333334,
      "loss": 0.0884,
      "step": 544
    },
    {
      "epoch": 5.46,
      "grad_norm": 0.8090130090713501,
      "learning_rate": 0.00025222222222222226,
      "loss": 0.1683,
      "step": 546
    },
    {
      "epoch": 5.48,
      "grad_norm": 0.9898756742477417,
      "learning_rate": 0.00025111111111111113,
      "loss": 0.1433,
      "step": 548
    },
    {
      "epoch": 5.5,
      "grad_norm": 0.7969157695770264,
      "learning_rate": 0.00025,
      "loss": 0.1485,
      "step": 550
    },
    {
      "epoch": 5.52,
      "grad_norm": 0.8201767802238464,
      "learning_rate": 0.0002488888888888889,
      "loss": 0.0904,
      "step": 552
    },
    {
      "epoch": 5.54,
      "grad_norm": 0.9708493947982788,
      "learning_rate": 0.0002477777777777778,
      "loss": 0.1234,
      "step": 554
    },
    {
      "epoch": 5.56,
      "grad_norm": 1.4145855903625488,
      "learning_rate": 0.0002466666666666667,
      "loss": 0.1938,
      "step": 556
    },
    {
      "epoch": 5.58,
      "grad_norm": 0.8012409210205078,
      "learning_rate": 0.00024555555555555556,
      "loss": 0.1111,
      "step": 558
    },
    {
      "epoch": 5.6,
      "grad_norm": 0.824088454246521,
      "learning_rate": 0.00024444444444444443,
      "loss": 0.1316,
      "step": 560
    },
    {
      "epoch": 5.62,
      "grad_norm": 0.8107313513755798,
      "learning_rate": 0.00024333333333333336,
      "loss": 0.1068,
      "step": 562
    },
    {
      "epoch": 5.64,
      "grad_norm": 1.2069036960601807,
      "learning_rate": 0.00024222222222222223,
      "loss": 0.1398,
      "step": 564
    },
    {
      "epoch": 5.66,
      "grad_norm": 1.0326005220413208,
      "learning_rate": 0.0002411111111111111,
      "loss": 0.1393,
      "step": 566
    },
    {
      "epoch": 5.68,
      "grad_norm": 0.9175305366516113,
      "learning_rate": 0.00024,
      "loss": 0.1352,
      "step": 568
    },
    {
      "epoch": 5.7,
      "grad_norm": 0.7314549088478088,
      "learning_rate": 0.0002388888888888889,
      "loss": 0.1354,
      "step": 570
    },
    {
      "epoch": 5.72,
      "grad_norm": 0.9895020723342896,
      "learning_rate": 0.00023777777777777778,
      "loss": 0.1442,
      "step": 572
    },
    {
      "epoch": 5.74,
      "grad_norm": 0.8936411142349243,
      "learning_rate": 0.00023666666666666668,
      "loss": 0.1198,
      "step": 574
    },
    {
      "epoch": 5.76,
      "grad_norm": 0.8720058798789978,
      "learning_rate": 0.00023555555555555556,
      "loss": 0.1597,
      "step": 576
    },
    {
      "epoch": 5.78,
      "grad_norm": 0.7379066348075867,
      "learning_rate": 0.00023444444444444446,
      "loss": 0.1415,
      "step": 578
    },
    {
      "epoch": 5.8,
      "grad_norm": 0.8705465197563171,
      "learning_rate": 0.00023333333333333333,
      "loss": 0.1071,
      "step": 580
    },
    {
      "epoch": 5.82,
      "grad_norm": 0.9963319301605225,
      "learning_rate": 0.00023222222222222223,
      "loss": 0.151,
      "step": 582
    },
    {
      "epoch": 5.84,
      "grad_norm": 0.983415424823761,
      "learning_rate": 0.0002311111111111111,
      "loss": 0.1551,
      "step": 584
    },
    {
      "epoch": 5.86,
      "grad_norm": 0.9211297035217285,
      "learning_rate": 0.00023,
      "loss": 0.1329,
      "step": 586
    },
    {
      "epoch": 5.88,
      "grad_norm": 0.9858004450798035,
      "learning_rate": 0.0002288888888888889,
      "loss": 0.107,
      "step": 588
    },
    {
      "epoch": 5.9,
      "grad_norm": 0.9262171983718872,
      "learning_rate": 0.00022777777777777778,
      "loss": 0.1447,
      "step": 590
    },
    {
      "epoch": 5.92,
      "grad_norm": 1.123764157295227,
      "learning_rate": 0.00022666666666666666,
      "loss": 0.1297,
      "step": 592
    },
    {
      "epoch": 5.94,
      "grad_norm": 0.9776748418807983,
      "learning_rate": 0.00022555555555555556,
      "loss": 0.1008,
      "step": 594
    },
    {
      "epoch": 5.96,
      "grad_norm": 0.9128952622413635,
      "learning_rate": 0.00022444444444444446,
      "loss": 0.1227,
      "step": 596
    },
    {
      "epoch": 5.98,
      "grad_norm": 1.1481105089187622,
      "learning_rate": 0.00022333333333333333,
      "loss": 0.1802,
      "step": 598
    },
    {
      "epoch": 6.0,
      "grad_norm": 1.0574499368667603,
      "learning_rate": 0.0002222222222222222,
      "loss": 0.1274,
      "step": 600
    },
    {
      "epoch": 6.02,
      "grad_norm": 0.7319404482841492,
      "learning_rate": 0.00022111111111111113,
      "loss": 0.088,
      "step": 602
    },
    {
      "epoch": 6.04,
      "grad_norm": 0.6448981165885925,
      "learning_rate": 0.00022,
      "loss": 0.0745,
      "step": 604
    },
    {
      "epoch": 6.06,
      "grad_norm": 0.7854152917861938,
      "learning_rate": 0.00021888888888888888,
      "loss": 0.0864,
      "step": 606
    },
    {
      "epoch": 6.08,
      "grad_norm": 1.3122200965881348,
      "learning_rate": 0.00021777777777777776,
      "loss": 0.1075,
      "step": 608
    },
    {
      "epoch": 6.1,
      "grad_norm": 0.6981473565101624,
      "learning_rate": 0.00021666666666666668,
      "loss": 0.0817,
      "step": 610
    },
    {
      "epoch": 6.12,
      "grad_norm": 1.0666985511779785,
      "learning_rate": 0.00021555555555555556,
      "loss": 0.0854,
      "step": 612
    },
    {
      "epoch": 6.14,
      "grad_norm": 0.8894607424736023,
      "learning_rate": 0.00021444444444444443,
      "loss": 0.09,
      "step": 614
    },
    {
      "epoch": 6.16,
      "grad_norm": 0.6811472177505493,
      "learning_rate": 0.00021333333333333336,
      "loss": 0.0783,
      "step": 616
    },
    {
      "epoch": 6.18,
      "grad_norm": 0.8588171005249023,
      "learning_rate": 0.00021222222222222223,
      "loss": 0.1033,
      "step": 618
    },
    {
      "epoch": 6.2,
      "grad_norm": 0.971239447593689,
      "learning_rate": 0.0002111111111111111,
      "loss": 0.0985,
      "step": 620
    },
    {
      "epoch": 6.22,
      "grad_norm": 0.747567355632782,
      "learning_rate": 0.00021,
      "loss": 0.0924,
      "step": 622
    },
    {
      "epoch": 6.24,
      "grad_norm": 0.4857923090457916,
      "learning_rate": 0.0002088888888888889,
      "loss": 0.0572,
      "step": 624
    },
    {
      "epoch": 6.26,
      "grad_norm": 0.5608347654342651,
      "learning_rate": 0.00020777777777777778,
      "loss": 0.0619,
      "step": 626
    },
    {
      "epoch": 6.28,
      "grad_norm": 0.5943054556846619,
      "learning_rate": 0.00020666666666666666,
      "loss": 0.063,
      "step": 628
    },
    {
      "epoch": 6.3,
      "grad_norm": 1.0180686712265015,
      "learning_rate": 0.00020555555555555556,
      "loss": 0.1025,
      "step": 630
    },
    {
      "epoch": 6.32,
      "grad_norm": 1.0318024158477783,
      "learning_rate": 0.00020444444444444446,
      "loss": 0.1076,
      "step": 632
    },
    {
      "epoch": 6.34,
      "grad_norm": 0.6079602837562561,
      "learning_rate": 0.00020333333333333333,
      "loss": 0.0859,
      "step": 634
    },
    {
      "epoch": 6.36,
      "grad_norm": 0.9822670221328735,
      "learning_rate": 0.00020222222222222223,
      "loss": 0.1014,
      "step": 636
    },
    {
      "epoch": 6.38,
      "grad_norm": 0.5564295053482056,
      "learning_rate": 0.0002011111111111111,
      "loss": 0.0658,
      "step": 638
    },
    {
      "epoch": 6.4,
      "grad_norm": 0.8492588996887207,
      "learning_rate": 0.0002,
      "loss": 0.0922,
      "step": 640
    },
    {
      "epoch": 6.42,
      "grad_norm": 0.777965784072876,
      "learning_rate": 0.00019888888888888888,
      "loss": 0.1095,
      "step": 642
    },
    {
      "epoch": 6.44,
      "grad_norm": 0.6085584759712219,
      "learning_rate": 0.00019777777777777778,
      "loss": 0.0799,
      "step": 644
    },
    {
      "epoch": 6.46,
      "grad_norm": 0.7863003611564636,
      "learning_rate": 0.00019666666666666666,
      "loss": 0.0948,
      "step": 646
    },
    {
      "epoch": 6.48,
      "grad_norm": 1.0541995763778687,
      "learning_rate": 0.00019555555555555556,
      "loss": 0.0767,
      "step": 648
    },
    {
      "epoch": 6.5,
      "grad_norm": 0.9789543151855469,
      "learning_rate": 0.00019444444444444446,
      "loss": 0.0981,
      "step": 650
    },
    {
      "epoch": 6.52,
      "grad_norm": 0.657502293586731,
      "learning_rate": 0.00019333333333333333,
      "loss": 0.0559,
      "step": 652
    },
    {
      "epoch": 6.54,
      "grad_norm": 1.1410304307937622,
      "learning_rate": 0.0001922222222222222,
      "loss": 0.0846,
      "step": 654
    },
    {
      "epoch": 6.56,
      "grad_norm": 0.9083174467086792,
      "learning_rate": 0.00019111111111111114,
      "loss": 0.0952,
      "step": 656
    },
    {
      "epoch": 6.58,
      "grad_norm": 1.038103461265564,
      "learning_rate": 0.00019,
      "loss": 0.0968,
      "step": 658
    },
    {
      "epoch": 6.6,
      "grad_norm": 0.6332471966743469,
      "learning_rate": 0.00018888888888888888,
      "loss": 0.0573,
      "step": 660
    },
    {
      "epoch": 6.62,
      "grad_norm": 0.8336901664733887,
      "learning_rate": 0.00018777777777777776,
      "loss": 0.0974,
      "step": 662
    },
    {
      "epoch": 6.64,
      "grad_norm": 1.0268787145614624,
      "learning_rate": 0.0001866666666666667,
      "loss": 0.076,
      "step": 664
    },
    {
      "epoch": 6.66,
      "grad_norm": 0.7714173793792725,
      "learning_rate": 0.00018555555555555556,
      "loss": 0.0811,
      "step": 666
    },
    {
      "epoch": 6.68,
      "grad_norm": 0.9876179099082947,
      "learning_rate": 0.00018444444444444443,
      "loss": 0.0749,
      "step": 668
    },
    {
      "epoch": 6.7,
      "grad_norm": 0.7351031303405762,
      "learning_rate": 0.00018333333333333334,
      "loss": 0.1063,
      "step": 670
    },
    {
      "epoch": 6.72,
      "grad_norm": 1.2654653787612915,
      "learning_rate": 0.00018222222222222224,
      "loss": 0.0877,
      "step": 672
    },
    {
      "epoch": 6.74,
      "grad_norm": 0.8879339098930359,
      "learning_rate": 0.0001811111111111111,
      "loss": 0.0916,
      "step": 674
    },
    {
      "epoch": 6.76,
      "grad_norm": 0.7306696772575378,
      "learning_rate": 0.00017999999999999998,
      "loss": 0.0746,
      "step": 676
    },
    {
      "epoch": 6.78,
      "grad_norm": 1.266516089439392,
      "learning_rate": 0.00017888888888888889,
      "loss": 0.0965,
      "step": 678
    },
    {
      "epoch": 6.8,
      "grad_norm": 0.8187850713729858,
      "learning_rate": 0.00017777777777777779,
      "loss": 0.0801,
      "step": 680
    },
    {
      "epoch": 6.82,
      "grad_norm": 0.6788027882575989,
      "learning_rate": 0.00017666666666666666,
      "loss": 0.0895,
      "step": 682
    },
    {
      "epoch": 6.84,
      "grad_norm": 1.140431523323059,
      "learning_rate": 0.00017555555555555556,
      "loss": 0.1169,
      "step": 684
    },
    {
      "epoch": 6.86,
      "grad_norm": 0.7900303602218628,
      "learning_rate": 0.00017444444444444446,
      "loss": 0.0981,
      "step": 686
    },
    {
      "epoch": 6.88,
      "grad_norm": 1.3543373346328735,
      "learning_rate": 0.00017333333333333334,
      "loss": 0.0915,
      "step": 688
    },
    {
      "epoch": 6.9,
      "grad_norm": 1.1729227304458618,
      "learning_rate": 0.00017222222222222224,
      "loss": 0.0943,
      "step": 690
    },
    {
      "epoch": 6.92,
      "grad_norm": 0.7075755000114441,
      "learning_rate": 0.0001711111111111111,
      "loss": 0.0881,
      "step": 692
    },
    {
      "epoch": 6.94,
      "grad_norm": 1.014481782913208,
      "learning_rate": 0.00017,
      "loss": 0.0669,
      "step": 694
    },
    {
      "epoch": 6.96,
      "grad_norm": 1.1112309694290161,
      "learning_rate": 0.00016888888888888889,
      "loss": 0.0766,
      "step": 696
    },
    {
      "epoch": 6.98,
      "grad_norm": 0.7321998476982117,
      "learning_rate": 0.0001677777777777778,
      "loss": 0.0829,
      "step": 698
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.9485252499580383,
      "learning_rate": 0.00016666666666666666,
      "loss": 0.0896,
      "step": 700
    },
    {
      "epoch": 7.02,
      "grad_norm": 0.6318458318710327,
      "learning_rate": 0.00016555555555555556,
      "loss": 0.0551,
      "step": 702
    },
    {
      "epoch": 7.04,
      "grad_norm": 1.6655365228652954,
      "learning_rate": 0.00016444444444444446,
      "loss": 0.0693,
      "step": 704
    },
    {
      "epoch": 7.06,
      "grad_norm": 0.5644461512565613,
      "learning_rate": 0.00016333333333333334,
      "loss": 0.0604,
      "step": 706
    },
    {
      "epoch": 7.08,
      "grad_norm": 0.6311626434326172,
      "learning_rate": 0.0001622222222222222,
      "loss": 0.0604,
      "step": 708
    },
    {
      "epoch": 7.1,
      "grad_norm": 0.7355284094810486,
      "learning_rate": 0.0001611111111111111,
      "loss": 0.0655,
      "step": 710
    },
    {
      "epoch": 7.12,
      "grad_norm": 0.6915493011474609,
      "learning_rate": 0.00016,
      "loss": 0.0548,
      "step": 712
    },
    {
      "epoch": 7.14,
      "grad_norm": 0.6622174978256226,
      "learning_rate": 0.0001588888888888889,
      "loss": 0.0529,
      "step": 714
    },
    {
      "epoch": 7.16,
      "grad_norm": 0.802139163017273,
      "learning_rate": 0.00015777777777777776,
      "loss": 0.0466,
      "step": 716
    },
    {
      "epoch": 7.18,
      "grad_norm": 0.5335490703582764,
      "learning_rate": 0.0001566666666666667,
      "loss": 0.0496,
      "step": 718
    },
    {
      "epoch": 7.2,
      "grad_norm": 0.7992636561393738,
      "learning_rate": 0.00015555555555555556,
      "loss": 0.0618,
      "step": 720
    },
    {
      "epoch": 7.22,
      "grad_norm": 0.5925252437591553,
      "learning_rate": 0.00015444444444444444,
      "loss": 0.0629,
      "step": 722
    },
    {
      "epoch": 7.24,
      "grad_norm": 0.4068198502063751,
      "learning_rate": 0.00015333333333333334,
      "loss": 0.0503,
      "step": 724
    },
    {
      "epoch": 7.26,
      "grad_norm": 0.9217114448547363,
      "learning_rate": 0.00015222222222222224,
      "loss": 0.0636,
      "step": 726
    },
    {
      "epoch": 7.28,
      "grad_norm": 0.4716305136680603,
      "learning_rate": 0.0001511111111111111,
      "loss": 0.0382,
      "step": 728
    },
    {
      "epoch": 7.3,
      "grad_norm": 0.6729540824890137,
      "learning_rate": 0.00015,
      "loss": 0.0575,
      "step": 730
    },
    {
      "epoch": 7.32,
      "grad_norm": 0.44188639521598816,
      "learning_rate": 0.0001488888888888889,
      "loss": 0.0446,
      "step": 732
    },
    {
      "epoch": 7.34,
      "grad_norm": 0.6431816816329956,
      "learning_rate": 0.0001477777777777778,
      "loss": 0.0629,
      "step": 734
    },
    {
      "epoch": 7.36,
      "grad_norm": 1.031262993812561,
      "learning_rate": 0.00014666666666666666,
      "loss": 0.0634,
      "step": 736
    },
    {
      "epoch": 7.38,
      "grad_norm": 0.7491945028305054,
      "learning_rate": 0.00014555555555555556,
      "loss": 0.0654,
      "step": 738
    },
    {
      "epoch": 7.4,
      "grad_norm": 0.4932629466056824,
      "learning_rate": 0.00014444444444444444,
      "loss": 0.0468,
      "step": 740
    },
    {
      "epoch": 7.42,
      "grad_norm": 0.9522885084152222,
      "learning_rate": 0.00014333333333333334,
      "loss": 0.0756,
      "step": 742
    },
    {
      "epoch": 7.44,
      "grad_norm": 0.7128097414970398,
      "learning_rate": 0.0001422222222222222,
      "loss": 0.0602,
      "step": 744
    },
    {
      "epoch": 7.46,
      "grad_norm": 0.7881627678871155,
      "learning_rate": 0.00014111111111111111,
      "loss": 0.0619,
      "step": 746
    },
    {
      "epoch": 7.48,
      "grad_norm": 0.7818022966384888,
      "learning_rate": 0.00014000000000000001,
      "loss": 0.0494,
      "step": 748
    },
    {
      "epoch": 7.5,
      "grad_norm": 0.5026428699493408,
      "learning_rate": 0.0001388888888888889,
      "loss": 0.0532,
      "step": 750
    }
  ],
  "logging_steps": 2,
  "max_steps": 1000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 10,
  "total_flos": 2388706983936000.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
