import os
import torch
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    TrainingArguments, 
    Trainer
)
from peft import (
    LoraConfig, 
    get_peft_model
)

from datasets import Dataset

from data.training_data.basic_training_data import basic_training_data

# è½‰æ›è³‡æ–™å’Œæ¨™è¨˜å‡½æ•¸
def convert_to_instruction_format(data):
    instruction_data = []
    for item in data:
        instruction_item = {
            "instruction": item["question"],
            "input": "",
            "output": item["answer"],
            "category": item["category"]
        }
        instruction_data.append(instruction_item)
    return instruction_data

def tokenize_function(examples, tokenizer, max_length=512):
    prompts = [
        f"Instruction: {instr}\nOutput: {out}"
        for instr, out in zip(examples["instruction"], examples["output"])
    ]
    
    tokenized = tokenizer(
        prompts,
        padding="max_length",
        truncation=True,
        max_length=max_length,
        return_tensors="pt"
    )
    
    tokenized["labels"] = tokenized["input_ids"].clone()
    
    return tokenized

# ä¸»è¦ç¨‹åº
def main():
    print("é–‹å§‹æº–å‚™è¨“ç·´è³‡æ–™...")
    
    instruction_data = convert_to_instruction_format(basic_training_data)
    dataset = Dataset.from_list(instruction_data)

    model_path = "./data/model/tinyllama-local"
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # æ¨™è¨˜åŒ–è³‡æ–™
    tokenized_dataset = dataset.map(
        lambda examples: tokenize_function(examples, tokenizer),
        batched=True,
        remove_columns=dataset.column_names
    )
    
    # è¼‰å…¥æ¨¡å‹
    dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=dtype,
        device_map={"": "cpu"}
    )
    
    # è¨­å®š LoRA
    lora_config = LoraConfig(
        r=8,
        lora_alpha=16,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
        lora_dropout=0.1,
        bias="none",
        task_type="CAUSAL_LM"
    )
    
    model = get_peft_model(model, lora_config)

    # è¨­å®šè¨“ç·´åƒæ•¸
    output_dir = "./results"
    training_args = TrainingArguments(
        output_dir=output_dir,
        learning_rate=5e-4,  # ğŸŸ¢ **æé«˜å­¸ç¿’ç‡ï¼Œè®“å°æ•¸æ“šé›†æ›´å¿«æ”¶æ–‚**
        per_device_train_batch_size=1,  # âœ… CPU è¨˜æ†¶é«”å—é™ï¼Œbatch size è¨­ 1
        gradient_accumulation_steps=1,  # ğŸ”µ **CPU è¨“ç·´ä¸éœ€è¦æ¢¯åº¦ç´¯ç©**
        num_train_epochs=10,  # ğŸŸ¢ **100 ç­†æ•¸æ“šæ‡‰è©²å¢åŠ  Epoch**
        weight_decay=0.01,
        logging_steps=2,  # ğŸ”µ **æ¸›å°‘ loggingï¼Œé¿å…å½±éŸ¿ CPU è¨“ç·´**
        save_steps=10,
        warmup_ratio=0.1,  # ğŸŸ¢ **å°æ•¸æ“š warmup å¤šä¸€é»ï¼Œæé«˜ç©©å®šæ€§**
        save_total_limit=2,
        no_cuda=True,  # ğŸŸ¢ **ç¢ºä¿åªç”¨ CPU**
        fp16=False,  # ğŸ›‘ **CPU ä¸æ”¯æ´ FP16ï¼Œéœ€é—œé–‰**
        report_to="none"
    )

    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        tokenizer=tokenizer
    )

    # Early Stopping æ¢ä»¶
    best_loss = float("inf")  # è¨˜éŒ„æœ€ä½ Loss
    patience = 3  # å…è¨±å¤šå°‘å€‹ Epoch æ²’æœ‰æ”¹å–„
    counter = 0  # è¨˜éŒ„é€£çºŒæœªæ”¹å–„æ¬¡æ•¸

    print("é–‹å§‹è¨“ç·´...")
    
    for epoch in range(training_args.num_train_epochs):
        train_result = trainer.train(resume_from_checkpoint=False)
        current_loss = train_result.training_loss  # å–å¾—ç•¶å‰ Loss
        
        print(f"Epoch {epoch+1} æå¤±: {current_loss:.4f}")

        # æª¢æŸ¥ Loss æ˜¯å¦æ”¹å–„
        if current_loss < best_loss:
            best_loss = current_loss
            counter = 0  # é‡ç½®è¨ˆæ•¸å™¨
        else:
            counter += 1  # é€£çºŒæœªæ”¹å–„ +1

        # è‹¥è¶…é patienceï¼Œå‰‡æå‰åœæ­¢
        if counter >= patience:
            print(f"Loss æœªæ”¹å–„ {patience} å€‹ Epochï¼Œæå‰åœæ­¢è¨“ç·´ï¼")
            break

    # å„²å­˜æ¨¡å‹
    lora_output_dir = os.path.join(output_dir, "lora-weights")
    os.makedirs(lora_output_dir, exist_ok=True)
    model.save_pretrained(lora_output_dir)
    tokenizer.save_pretrained(lora_output_dir)

    print(f"è¨“ç·´å®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜è‡³: {lora_output_dir}")
    
    # æ¸¬è©¦æ¨¡å‹
    test_prompts = [
        "å¦‚ä½•é˜²æ­¢é‹å­ç™¼éœ‰ï¼Ÿ",
        "é‹å­ä¸Šçš„é»´èŒæœ‰ä»€éº¼å±å®³ï¼Ÿ"
    ]

    print("\né–‹å§‹æ¸¬è©¦æ¨¡å‹...")
    for prompt in test_prompts:
        print(f"\nå•é¡Œ: {prompt}")
        
        inputs = tokenizer(f"Instruction: {prompt}\nOutput:", return_tensors="pt")
        with torch.no_grad():
            outputs = model.generate(
                inputs["input_ids"],
                max_length=200,
                temperature=0.7,
                top_p=0.9,
                do_sample=True
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"å›æ‡‰: {response}")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"è¨“ç·´éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        import traceback
        traceback.print_exc()
